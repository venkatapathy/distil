{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8AxA7KVfv9m"
      },
      "source": [
        "# **DISTIL Usage Example: CIFAR10**\n",
        "\n",
        "Here, we show how to use DISTIL to perform active learning on image classification tasks (CIFAR10). This notebook can be easily executed on Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4Y2r9Y_fH5B"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9bsVDupO_EOh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/venkat/pyvenvs/distil/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import cifar\n",
        "\n",
        "sys.path.append('/home/venkat/distil/')\n",
        "\n",
        "from distil.active_learning_strategies import GLISTER, BADGE, EntropySampling, RandomSampling   # All active learning strategies showcased in this example\n",
        "from distil.active_learning_strategies.wassal import WASSAL\n",
        "from distil.utils.models.resnet import ResNet18                                                 # The model used in our image classification example\n",
        "from distil.utils.train_helper import data_train                                                # A utility training class provided by DISTIL\n",
        "from distil.utils.utils import LabeledToUnlabeledDataset                                        # A utility wrapper class that removes labels from labeled PyTorch dataset objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvn_Dal4gb8S"
      },
      "source": [
        "## Preparing CIFAR10\n",
        "\n",
        "The CIFAR10 dataset contains 60,000 32x32 color images in 10 different classes.The 10 different classes represent airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. There are 6,000 images of each class. The training set contains 50,000 images, and the test set contains 10,000 images. Here, we do a simple setup of the CIFAR10 dataset that we will use in this example. More importantly, we define a split on CIFAR10's training set into an initial labeled seed set and an unlabeled set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jdLy9wyjCuT-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '.'\n",
        "\n",
        "# Define transforms on the dataset splits of CIFAR10. Here, we use random crops and horizontal flips for training augmentations.\n",
        "# Both the train and test sets are converted to PyTorch tensors and are normalized around the mean/std of CIFAR-10.\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "# Get the dataset objects from PyTorch. Here, CIFAR10 is downloaded, and the transform is applied when points \n",
        "# are retrieved.\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "# Get the dimension of the images. Here, we simply take the very first image of CIFAR10 \n",
        "# and query its dimension.\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "# We now define a train-unlabeled split for the sake of the experiment. Here, we simply take 1000 points as the initial seed set.\n",
        "# The rest of the points are taken as the unlabeled set. While the unlabeled set constructed here technically has labels, they \n",
        "# are only used when querying for labels. Hence, they only exist here for the sake of experimental design.\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(100)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, 2000)))\n",
        "\n",
        "# Define the number of active learning rounds to conduct, the budget, and the number of classes in CIFAR10\n",
        "nclasses = 10\n",
        "n_rounds = 9\n",
        "budget = 50 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVwJ7RcOhVc7"
      },
      "source": [
        "## Preparing the Model\n",
        "\n",
        "Here, we use DISTIL's provided implementation of the [ResNet-18](https://arxiv.org/abs/1512.03385) architecture. We also create a model directory to store trained models in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4pHzwNwvhVts"
      },
      "outputs": [],
      "source": [
        "net = ResNet18()\n",
        "base_dir = \"models\"\n",
        "os.makedirs(base_dir, exist_ok = True)\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spOTFeWgfm4b"
      },
      "source": [
        "## Training an Initial Model\n",
        "Here, we train an initial model. We do so by creating a training loop object on the initial seed set, the model architecture, and a list of provided arguments. We then save the initial model. Note: If you've already run the first cell, then you can simply run the second cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ru5tj6u8fwEi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training..\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m args \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn_epoch\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m300\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mfloat\u001b[39m(\u001b[39m0.01\u001b[39m), \u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m20\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmax_accuracy\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m0.99\u001b[39m, \u001b[39m'\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m} \n\u001b[1;32m      5\u001b[0m dt \u001b[39m=\u001b[39m data_train(cifar10_train, net, args)\n\u001b[0;32m----> 6\u001b[0m clf \u001b[39m=\u001b[39m dt\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      7\u001b[0m torch\u001b[39m.\u001b[39msave(clf\u001b[39m.\u001b[39mstate_dict(), model_directory)\n",
            "File \u001b[0;32m~/distil/distil/utils/train_helper.py:262\u001b[0m, in \u001b[0;36mdata_train.train\u001b[0;34m(self, gradient_weights)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[39mwhile\u001b[39;00m (accCurrent \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mmax_accuracy\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39mand\u001b[39;00m (epoch \u001b[39m<\u001b[39m n_epoch) \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_saturated): \n\u001b[1;32m    261\u001b[0m     \u001b[39mif\u001b[39;00m gradient_weights \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         accCurrent, lossCurrent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train(epoch, loader_tr, optimizer)\n\u001b[1;32m    263\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m         accCurrent, lossCurrent \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_weighted(epoch, loader_tr, optimizer, gradient_weights)\n",
            "File \u001b[0;32m~/distil/distil/utils/train_helper.py:185\u001b[0m, in \u001b[0;36mdata_train._train\u001b[0;34m(self, epoch, loader_tr, optimizer)\u001b[0m\n\u001b[1;32m    182\u001b[0m x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), y\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    184\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 185\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclf(x)\n\u001b[1;32m    186\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, y\u001b[39m.\u001b[39mlong())\n\u001b[1;32m    187\u001b[0m accFinal \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum((torch\u001b[39m.\u001b[39mmax(out,\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m y)\u001b[39m.\u001b[39mfloat())\u001b[39m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/distil/distil/utils/models/resnet.py:102\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x, last, freeze)\u001b[0m\n\u001b[1;32m    100\u001b[0m         e \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    101\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m    103\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(out)\n\u001b[1;32m    104\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Specify additional training parameters. Here, we set the maximum number of epochs of training to 300, \n",
        "# the learning rate to 0.01, the batch size to 20, the maximum train accuracy of training to 0.99, and \n",
        "# the optimizer to stochastic gradient descent.\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, net, args)\n",
        "clf = dt.train()\n",
        "torch.save(clf.state_dict(), model_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dd-ieb7RgyJY"
      },
      "outputs": [],
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsUx57BjBT2"
      },
      "source": [
        "## Active Learning Strategies\n",
        "\n",
        "Here, we show examples of a couple active learning strategies being used in the setting of image classification."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wassertein Distance based AL- WassAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Testing accuracy: 26.7\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "There are 1000 Unlabeled dataset\n",
            "length of unlabelled dataset 1000\n",
            "Totals Probability of the budget: tensor(0.9651, device='cuda:0', grad_fn=<SumBackward0>)\n",
            "Number of training points - 600\n",
            "Numer of unlabeled points - 1000\n",
            "Training..\n"
          ]
        }
      ],
      "source": [
        "from distil.active_learning_strategies.wassal import WASSAL\n",
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20,'wd_num_epochs':50,'wd_lr':0.02,'verbose':True}\n",
        "strategy = WASSAL(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    cifar10_full_train.transform = cifar_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    print('indexes of selected dataset',idx)\n",
        "    cifar10_full_train.transform = cifar_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "    print('Numer of unlabeled points -',len(cifar10_unlabeled))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'wassal.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Bxn6LmCeEI"
      },
      "source": [
        "### Random Sampling\n",
        "This strategy is often used as a baseline, where we pick a subset of unlabeled points randomly. Here, we create a instance of `distil.active_learning_strategies.random_sampling.RandomSampling` by passing following parameters:\n",
        "\n",
        "**training_dataset** – The labeled dataset, which has a PyTorch interface that returns a tensor of data and a tensor of labels OR a dictionary with a \"labels\" key.\n",
        "\n",
        "**unlabeled_dataset** – The unlabeled dataset, which has a PyTorch interface that returns a tensor of data OR a dictionary.\n",
        "\n",
        "**net** – Model architecture used for training. Could be instance of models defined in distil.utils.models or something similar (in this instance, a Huggingface transformer).\n",
        "\n",
        "**nclasses** – No. of classes in the dataset\n",
        "\n",
        "**args**– This dictionary should have ‘batch_size’ as a key. 'batch_size' should be such that one can exploit the benefits of tensorization while honouring the resourse constraits. This ‘batch_size’ therefore can be different than the one used for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8OdeQ63DZuP"
      },
      "outputs": [],
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = RandomSampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    cifar10_full_train.transform = cifar_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "# Lastly, we save the accuracies in case a comparison is warranted.\n",
        "with open(os.path.join(base_dir,'random.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jm_ILTinrkLk"
      },
      "source": [
        "### Entropy Sampling\n",
        "A very basic strategy to select unlabeled points is entropy sampling, where we select samples about which the model is most uncertain by measuring the entropy of the class prediction. Hence, a valid strategy is to select those points in the unlabeled set with highest entropy (maximum uncertainty). Specifically, let $z = f(x)$ be the output from the model with class $i$ having score $z_i$. By applying a softmax, we obtain probabilities that we can use: \n",
        "\n",
        "$$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$$ \n",
        "\n",
        "Using the softmax probabilities, entropy sampling selects a subset based on the following objective:\n",
        "\n",
        " $$\\text{argmin}_{A \\subseteq U, |A| = k} \\sum_{x \\in A} \\left[ -\\sum_j \\sigma(z)_j*\\log(\\sigma(z)_j) \\right]$$\n",
        "\n",
        "Here, we create a instance of `distil.active_learning_strategies.entropy_sampling.EntropySampling` with the same parameters passed to `distil.active_learning_strategies.random_sampling.RandomSampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieS3rs-TC8bE"
      },
      "source": [
        "**Reloading Base Model & Data**\n",
        "\n",
        "We make sure the fixture is the same by repeating the same setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p5sn-ejd2IW4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '.'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 100\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    \n",
        "budget = 50\n",
        "\n",
        "net = ResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bheCElpz2Q-5"
      },
      "outputs": [],
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_3C0729a2Vzg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Testing accuracy: 56.45\n",
            "-------------------------------------------------\n",
            "Round 1\n",
            "-------------------------------------------------\n",
            "Number of training points - 150\n",
            "Training..\n",
            "Epoch: 118 Training accuracy: 0.993\n",
            "Testing accuracy: 25.69\n",
            "-------------------------------------------------\n",
            "Round 2\n",
            "-------------------------------------------------\n",
            "Number of training points - 200\n",
            "Training..\n",
            "Epoch: 107 Training accuracy: 0.99\n",
            "Testing accuracy: 28.73\n",
            "-------------------------------------------------\n",
            "Round 3\n",
            "-------------------------------------------------\n",
            "Number of training points - 250\n",
            "Training..\n",
            "Epoch: 107 Training accuracy: 0.992\n",
            "Testing accuracy: 31.4\n",
            "-------------------------------------------------\n",
            "Round 4\n",
            "-------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# Use select() to obtain the indices in the unlabeled set that should be labeled\u001b[39;00m\n\u001b[1;32m     24\u001b[0m cifar10_full_train\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m cifar_test_transform       \u001b[39m# Disable augmentation while selecting new points as to not interfere with the strategies\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m idx \u001b[39m=\u001b[39m strategy\u001b[39m.\u001b[39;49mselect(budget)\n\u001b[1;32m     26\u001b[0m cifar10_full_train\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m cifar_training_transform   \u001b[39m# Enable augmentation\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# Add the selected points to the train set. The unlabeled set shown in the next couple lines \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# already has the associated labels, so no human labeling is needed. Again, this is because \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m# we already have the labels a priori. In real scenarios, a human oracle would need to provide \u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# then before proceeding.\u001b[39;00m\n",
            "File \u001b[0;32m~/distil/distil/active_learning_strategies/score_streaming_strategy.py:99\u001b[0m, in \u001b[0;36mScoreStreamingStrategy.select\u001b[0;34m(self, budget)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[39mwhile\u001b[39;00m evaluated_points \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munlabeled_dataset):\n\u001b[1;32m     98\u001b[0m     buffered_stream \u001b[39m=\u001b[39m Subset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munlabeled_dataset, \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(evaluated_points, \u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39munlabeled_dataset), evaluated_points \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream_buffer_size))))\n\u001b[0;32m---> 99\u001b[0m     batch_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49macquire_scores(buffered_stream)\n\u001b[1;32m    100\u001b[0m     batch_scores \u001b[39m=\u001b[39m [(x, i \u001b[39m+\u001b[39m evaluated_points) \u001b[39mfor\u001b[39;00m i,x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_scores)]\n\u001b[1;32m    101\u001b[0m     batch_scores \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(batch_scores, key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m0\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[0;32m~/distil/distil/active_learning_strategies/entropy_sampling.py:49\u001b[0m, in \u001b[0;36mEntropySampling.acquire_scores\u001b[0;34m(self, unlabeled_buffer)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39macquire_scores\u001b[39m(\u001b[39mself\u001b[39m, unlabeled_buffer):\n\u001b[0;32m---> 49\u001b[0m     probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_prob(unlabeled_buffer)\n\u001b[1;32m     50\u001b[0m     log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(probs)\n\u001b[1;32m     51\u001b[0m     U \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(probs\u001b[39m*\u001b[39mlog_probs)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/distil/distil/active_learning_strategies/strategy.py:112\u001b[0m, in \u001b[0;36mStrategy.predict_prob\u001b[0;34m(self, to_predict_dataset)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     elements_to_predict \u001b[39m=\u001b[39m elements_to_predict\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 112\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(elements_to_predict)\n\u001b[1;32m    113\u001b[0m pred \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(out, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    115\u001b[0m \u001b[39m# Insert the calculated batch of probabilities into the tensor to return\u001b[39;00m\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/distil/distil/utils/models/resnet.py:106\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x, last, freeze)\u001b[0m\n\u001b[1;32m    104\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(out)\n\u001b[1;32m    105\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(out)\n\u001b[0;32m--> 106\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(out)\n\u001b[1;32m    107\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mavg_pool2d(out, \u001b[39m4\u001b[39m)\n\u001b[1;32m    108\u001b[0m e \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/distil/distil/utils/models/resnet.py:32\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)))\n\u001b[1;32m     33\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out))\n\u001b[1;32m     34\u001b[0m     out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(x)\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/pyvenvs/distil/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = EntropySampling(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    cifar10_full_train.transform = cifar_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BAGqAV0GrwwN"
      },
      "source": [
        "### BADGE\n",
        "This method is based on the paper [Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds](https://arxiv.org/abs/1906.03671). The strategy is meant to select points that are both diverse (e.g., their embeddings span multiple directions) and uncertain (e.g., their contribution to the loss is large). The following steps are taken:\n",
        "\n",
        "* Calculate the pseudo-label for each point in the unlabeled set. The pseudo-label is the class with the highest probability.\n",
        "* Compute the cross-entropy loss for each point in the unlabeled set using this pseudo-label.\n",
        "* Obtain the resulting loss gradients on the last linear layer of the model for each point. (These are referred to as the hypothesized loss gradients.)\n",
        "* Using these gradients as a form of embedding for each unlabeled point, run k-means++ initialization on this embedding set, retrieving $k$ centers. Each center is a point from the unlabeled set, and $k$ represents the active learning budget.\n",
        "* Request labels for the $k$ points whose embeddings were selected.\n",
        "\n",
        "Here we create a instance of `distil.active_learning_strategies.badge.BADGE` with same parameters passed to `distil.active_learning_strategies.random_sampling.RandomSampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z2N1gsCGE8U"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voTthJxV2kLB"
      },
      "outputs": [],
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '.'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF0BAQAL2kLB"
      },
      "outputs": [],
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6x0xEWpl2kLC"
      },
      "outputs": [],
      "source": [
        "# Initialize the random sampling AL strategy. Note: The labels are shaved off the unlabeled dataset above to match the setting.\n",
        "strategy_args = {'batch_size' : 20}\n",
        "strategy = BADGE(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    cifar10_full_train.transform = cifar_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DuIa_yJdiDOh"
      },
      "source": [
        "### GLISTER\n",
        "This is implemetation of GLISTER-ACTIVE from the paper [GLISTER: Generalization based Data Subset Selection for Efficient and Robust Learning](https://arxiv.org/abs/2012.10630). `GLISTER` tries to solve the following bi-level optimisation problem:\n",
        "\\begin{equation*}\n",
        "\\overbrace{\\underset{{S \\subseteq {\\mathcal U}, |S| \\leq k}}{\\operatorname{argmax\\hspace{0.7mm}}} LL_V(\\underbrace{\\underset{\\theta}{\\operatorname{argmax\\hspace{0.7mm}}} LL_T( \\theta, S)}_{inner-level}, {\\mathcal V})}^{outer-level}\n",
        "\\end{equation*}\n",
        "where is $S$ is set of points selected at each round, ${\\mathcal V}$ could be a dedicated validation set with labeled points or could be union of labeled and unlabeled points with hypothesized labels, and $k$ is the budget.\n",
        "\n",
        "Solving this problem directly is almost intractable due to the bi-level nature (e.g., repeatedly having to solve the inner problem); therefore, GLISTER resorts to one-step approximations. To start, we set $S^0$ as the empty set and incrementally build it as $S^k = S^{k-1} \\cup e$, where $e$ is $\\underset{e}{\\operatorname{argmax\\hspace{0.7mm}}} G_{\\theta}(e | S^k)$, defined below: $$G_{\\theta}(e | S^k) = LL_{V}(\\theta^{k}, {\\mathcal V})$$\n",
        "\n",
        "Concurrently, GLISTER updates the model parameters via the aforementioned one-step approximation after each greedy selection, which allows for recalculation of $G_{\\theta}$:\n",
        "\n",
        "$$\\theta^k \\leftarrow \\theta^{k-1} + \\eta \\nabla_{\\theta} LL_T(\\hat{\\theta}, e),$$ \n",
        "\n",
        "where $\\hat{\\theta}$ are the parameters of the model at the begining of the selection.\n",
        "\n",
        "To prevent overfitting, we can add regularization to GLISTER, which can be set by **_typeOf_**. **_typeOf_** can be set to **None** for normal GLISTER (default), **'Rand'** for replacing **_lam_** fraction of points with random points, **'Diversity'** for adding a diversity set function while computing gain, and **'FacLoc'** for adding a facility location set function while computing gain. **_lam_** for both **'Diversity'** and **'FacLoc'** determines the weightage given to them while computing the gain.\n",
        "\n",
        "Here, we create a instance of `distil.active_learning_strategies.glister.GLISTER` with the same parameters passed to `distil.active_learning_strategies.random_sampling.RandomSampling`. Additionally, we add to the **_args_** dictionary the required keys ‘batch_size’ and ‘lr’. ‘lr’ should be the learning rate used for training. Lastly, the following parameters can be passed as previously described:\n",
        "\n",
        "**validation_dataset (torch.utils.data.Dataset, optional)** – An optional validation dataset.\n",
        "\n",
        "**typeOf (str, optional)** – Determines the type of regularizer to be used. The default is **None**. For a random regularizer, use **‘Rand’**. To use the facility location set function as a regularizer, use **‘FacLoc’**. To use a diversity set function as a regularizer, use **‘Diversity’**.\n",
        "\n",
        "**lam (float, optional)** – Determines the amount of regularization to be applied. Mandatory if is not **_typeOf_=None** and, by default, is set to **None**. For random regularizers, use values between 0 and 1 as it determines fraction of points replaced by random points. For both **‘Diversity’** and **‘FacLoc’**, **_lam_** determines the weightage given to them while computing the gain.\n",
        "\n",
        "**kernel_batch_size (int, optional)** – For the **'Diversity'** and **'FacLoc'** regularizer versions, a similarity kernel is computed, which entails creating a 3-D Torch tensor of dimensions $kernel\\_batch\\_size^{2}\\times(feature\\ dimension)$. Again, **_kernel_batch_size_** should be such that one can exploit the benefits of tensorization while honouring the resourse constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hfo3xvHGJog"
      },
      "source": [
        "**Reloading Base Model & Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaVPq19w2sWH"
      },
      "outputs": [],
      "source": [
        "data_set_name = 'CIFAR10'\n",
        "download_path = '.'\n",
        "\n",
        "cifar_training_transform = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_test_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
        "cifar_target_transform = transforms.ToTensor()\n",
        "\n",
        "cifar10_full_train = cifar.CIFAR10(download_path, train=True, download=True, transform=cifar_training_transform, target_transform=torch.tensor)\n",
        "cifar10_test = cifar.CIFAR10(download_path, train=False, download=True, transform=cifar_test_transform, target_transform=torch.tensor)\n",
        "\n",
        "dim = np.shape(cifar10_full_train[0][0])\n",
        "\n",
        "train_size = 1000\n",
        "cifar10_train = Subset(cifar10_full_train, list(range(train_size)))\n",
        "cifar10_unlabeled = Subset(cifar10_full_train, list(range(train_size, len(cifar10_full_train))))\n",
        "\n",
        "nclasses = 10\n",
        "n_rounds = 9    ##Number of rounds to run active learning\n",
        "budget = 500 \n",
        "\n",
        "net = ResNet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbkv74Cq2sWH"
      },
      "outputs": [],
      "source": [
        "base_dir = \"models\"\n",
        "model_directory = os.path.join(base_dir, 'base_model.pth')\n",
        "net.load_state_dict(torch.load(model_directory))\n",
        "clf = net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFg2EPMC2sWH"
      },
      "outputs": [],
      "source": [
        "#Initializing Strategy Class\n",
        "strategy_args = {'batch_size' : 20, 'lr' : 0.01}\n",
        "strategy = GLISTER(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled), net, nclasses, strategy_args)\n",
        "\n",
        "# Use the same training parameters as before\n",
        "args = {'n_epoch':300, 'lr':float(0.01), 'batch_size':20, 'max_accuracy':0.99, 'optimizer':'sgd'} \n",
        "dt = data_train(cifar10_train, clf, args)\n",
        "\n",
        "# Update the model used in the AL strategy with the loaded initial model\n",
        "strategy.update_model(clf)\n",
        "\n",
        "# Get the test accuracy of the initial model\n",
        "acc = np.zeros(n_rounds)\n",
        "acc[0] = dt.get_acc_on_set(cifar10_test)\n",
        "print('Initial Testing accuracy:', round(acc[0]*100, 2), flush=True)\n",
        "\n",
        "# User Controlled Loop\n",
        "for rd in range(1, n_rounds):\n",
        "    print('-------------------------------------------------')\n",
        "    print('Round', rd) \n",
        "    print('-------------------------------------------------')\n",
        "\n",
        "    # Use select() to obtain the indices in the unlabeled set that should be labeled\n",
        "    cifar10_full_train.transform = cifar_test_transform       # Disable augmentation while selecting new points as to not interfere with the strategies\n",
        "    idx = strategy.select(budget)\n",
        "    cifar10_full_train.transform = cifar_training_transform   # Enable augmentation\n",
        "\n",
        "    # Add the selected points to the train set. The unlabeled set shown in the next couple lines \n",
        "    # already has the associated labels, so no human labeling is needed. Again, this is because \n",
        "    # we already have the labels a priori. In real scenarios, a human oracle would need to provide \n",
        "    # then before proceeding.\n",
        "    cifar10_train = ConcatDataset([cifar10_train, Subset(cifar10_unlabeled, idx)])\n",
        "    remaining_unlabeled_idx = list(set(range(len(cifar10_unlabeled))) - set(idx))\n",
        "    cifar10_unlabeled = Subset(cifar10_unlabeled, remaining_unlabeled_idx)\n",
        "\n",
        "    print('Number of training points -', len(cifar10_train))\n",
        "\n",
        "    # Update the data used in the AL strategy and the training class\n",
        "    strategy.update_data(cifar10_train, LabeledToUnlabeledDataset(cifar10_unlabeled))\n",
        "    dt.update_data(cifar10_train)\n",
        "\n",
        "    # Retrain the model and update the strategy with the result\n",
        "    clf = dt.train()\n",
        "    strategy.update_model(clf)\n",
        "\n",
        "    # Get new test accuracy\n",
        "    acc[rd] = dt.get_acc_on_set(cifar10_test)\n",
        "    print('Testing accuracy:', round(acc[rd]*100, 2), flush=True)\n",
        "\n",
        "print('Training Completed')\n",
        "\n",
        "#Saving accuracies for further analysis\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'w') as f:\n",
        "    for item in acc:\n",
        "        f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNZjPPO7java"
      },
      "source": [
        "## Visualizing the Results\n",
        "\n",
        "If all strategies have run to completion, you can run the following cell to view the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vn65GnhZosp"
      },
      "outputs": [],
      "source": [
        "# Load the accuracies previously obtained\n",
        "with open(os.path.join(base_dir,'entropy.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_en = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'badge.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_bd = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'glister.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_gl = [round(float(x)*100, 2) for x in acc_]\n",
        "with open(os.path.join(base_dir,'random.txt'), 'r') as f:\n",
        "  acc_ = f.readlines()\n",
        "acc_rd = [round(float(x)*100, 2) for x in acc_]\n",
        "\n",
        "# Plot them using matplotlib\n",
        "x_axis = np.array([train_size+budget*i for i in range(n_rounds)])\n",
        "plt.figure()\n",
        "plt.plot(x_axis, acc_gl, 'b-', label='GLISTER RAND',marker='o')\n",
        "plt.plot(x_axis, acc_en, 'g-', label='UNCERTAINITY',marker='o')\n",
        "plt.plot(x_axis, acc_bd, 'c', label='BADGE',marker='o')\n",
        "plt.plot(x_axis, acc_rd, 'r', label='RANDOM',marker='o')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('No of Images')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.title('DISTIL_CIFAR10')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WyIVzbwQgrwN",
        "W-Bxn6LmCeEI",
        "jm_ILTinrkLk",
        "BAGqAV0GrwwN"
      ],
      "name": "DISTIL Example CIFAR10.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "distil",
      "language": "python",
      "name": "distil"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
